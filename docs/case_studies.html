<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Chapter&nbsp;1.&nbsp;Apache HBase (TM) Case Studies</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter" title="Chapter&nbsp;1.&nbsp;Apache HBase (TM) Case Studies"><div class="titlepage"><div><div><h2 class="title"><a name="casestudies"></a>Chapter&nbsp;1.&nbsp;Apache HBase (TM) Case Studies</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#casestudies.overview">1.1. Overview</a></span></dt><dt><span class="section"><a href="#casestudies.schema">1.2. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="#casestudies.schema.listdata">1.2.1. List Data</a></span></dt></dl></dd><dt><span class="section"><a href="#casestudies.perftroub">1.3. Performance/Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="#casestudies.slownode">1.3.1. Case Study #1 (Performance Issue On A Single Node)</a></span></dt><dt><span class="section"><a href="#casestudies.perf.1">1.3.2. Case Study #2 (Performance Research 2012)</a></span></dt><dt><span class="section"><a href="#casestudies.perf.2">1.3.3. Case Study #3 (Performance Research 2010))</a></span></dt><dt><span class="section"><a href="#casestudies.xceivers">1.3.4. Case Study #4 (xcievers Config)</a></span></dt></dl></dd></dl></div><div class="section" title="1.1.&nbsp;Overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.overview"></a>1.1.&nbsp;Overview</h2></div></div></div><p>This chapter will describe a variety of performance and troubleshooting case studies that can 
      provide a useful blueprint on diagnosing Apache HBase (TM) cluster issues.</p><p>For more information on Performance and Troubleshooting, see <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a>.
      </p></div><div class="section" title="1.2.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.schema"></a>1.2.&nbsp;Schema Design</h2></div></div></div><div class="section" title="1.2.1.&nbsp;List Data"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.schema.listdata"></a>1.2.1.&nbsp;List Data</h3></div></div></div><p>The following is an exchange from the user dist-list regarding a fairly common question:  
    		how to handle per-user list data in Apache HBase. 
    		</p><p>*** QUESTION ***</p><p>
    		We're looking at how to store a large amount of (per-user) list data in
HBase, and we were trying to figure out what kind of access pattern made
the most sense.  One option is store the majority of the data in a key, so
we could have something like:
    		</p><pre class="programlisting">
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId1&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId2&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId3&gt;:"" (no value)
			</pre>

The other option we had was to do this entirely using:
    		<pre class="programlisting">
&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum0&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum1&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
    		</pre><p>
where each row would contain multiple values.
So in one case reading the first thirty values would be:
			</p><pre class="programlisting">
scan { STARTROW =&gt; 'FixedWidthUsername' LIMIT =&gt; 30}
    		</pre>
And in the second case it would be
    		<pre class="programlisting">
get 'FixedWidthUserName\x00\x00\x00\x00'
    		</pre><p>
The general usage pattern would be to read only the first 30 values of
these lists, with infrequent access reading deeper into the lists.  Some
users would have &lt;= 30 total values in these lists, and some users would
have millions (i.e. power-law distribution)
			</p><p>
 The single-value format seems like it would take up more space on HBase,
but would offer some improved retrieval / pagination flexibility.  Would
there be any significant performance advantages to be able to paginate via
gets vs paginating with scans?
			</p><p>
  My initial understanding was that doing a scan should be faster if our
paging size is unknown (and caching is set appropriately), but that gets
should be faster if we'll always need the same page size.  I've ended up
hearing different people tell me opposite things about performance.  I
assume the page sizes would be relatively consistent, so for most use cases
we could guarantee that we only wanted one page of data in the
fixed-page-length case.  I would also assume that we would have infrequent
updates, but may have inserts into the middle of these lists (meaning we'd
need to update all subsequent rows).
			</p><p>
Thanks for help / suggestions / follow-up questions.
			</p><p>*** ANSWER ***</p><p>
If I understand you correctly, you're ultimately trying to store
triples in the form "user, valueid, value", right? E.g., something
like:
			</p><pre class="programlisting">
"user123, firstname, Paul",
"user234, lastname, Smith"
			</pre><p>
(But the usernames are fixed width, and the valueids are fixed width).
			</p><p>
And, your access pattern is along the lines of: "for user X, list the
next 30 values, starting with valueid Y". Is that right? And these
values should be returned sorted by valueid?
			</p><p>
The tl;dr version is that you should probably go with one row per
user+value, and not build a complicated intra-row pagination scheme on
your own unless you're really sure it is needed.
			</p><p>
Your two options mirror a common question people have when designing
HBase schemas: should I go "tall" or "wide"? Your first schema is
"tall": each row represents one value for one user, and so there are
many rows in the table for each user; the row key is user + valueid,
and there would be (presumably) a single column qualifier that means
"the value". This is great if you want to scan over rows in sorted
order by row key (thus my question above, about whether these ids are
sorted correctly). You can start a scan at any user+valueid, read the
next 30, and be done. What you're giving up is the ability to have
transactional guarantees around all the rows for one user, but it
doesn't sound like you need that. Doing it this way is generally
recommended (see
here <a class="link" href="http://hbase.apache.org/book.html#schema.smackdown" target="_top">http://hbase.apache.org/book.html#schema.smackdown</a>).
			</p><p>
Your second option is "wide": you store a bunch of values in one row,
using different qualifiers (where the qualifier is the valueid). The
simple way to do that would be to just store ALL values for one user
in a single row. I'm guessing you jumped to the "paginated" version
because you're assuming that storing millions of columns in a single
row would be bad for performance, which may or may not be true; as
long as you're not trying to do too much in a single request, or do
things like scanning over and returning all of the cells in the row,
it shouldn't be fundamentally worse. The client has methods that allow
you to get specific slices of columns.
			</p><p>
Note that neither case fundamentally uses more disk space than the
other; you're just "shifting" part of the identifying information for
a value either to the left (into the row key, in option one) or to the
right (into the column qualifiers in option 2). Under the covers,
every key/value still stores the whole row key, and column family
name. (If this is a bit confusing, take an hour and watch Lars
George's excellent video about understanding HBase schema design:
<a class="link" href="http://www.youtube.com/watch?v=_HLoH_PgrLk)" target="_top">http://www.youtube.com/watch?v=_HLoH_PgrLk)</a>.
			</p><p>
A manually paginated version has lots more complexities, as you note,
like having to keep track of how many things are in each page,
re-shuffling if new values are inserted, etc. That seems significantly
more complex. It might have some slight speed advantages (or
disadvantages!) at extremely high throughput, and the only way to
really know that would be to try it out. If you don't have time to
build it both ways and compare, my advice would be to start with the
simplest option (one row per user+value). Start simple and iterate! :)
			</p></div></div><div class="section" title="1.3.&nbsp;Performance/Troubleshooting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.perftroub"></a>1.3.&nbsp;Performance/Troubleshooting</h2></div></div></div><div class="section" title="1.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.slownode"></a>1.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)</h3></div></div></div><div class="section" title="1.3.1.1.&nbsp;Scenario"><div class="titlepage"><div><div><h4 class="title"><a name="d816e82"></a>1.3.1.1.&nbsp;Scenario</h4></div></div></div><p>Following a scheduled reboot, one data node began exhibiting unusual behavior.  Routine MapReduce 
         jobs run against HBase tables which regularly completed in five or six minutes began taking 30 or 40 minutes 
         to finish. These jobs were consistently found to be waiting on map and reduce tasks assigned to the troubled data node 
         (e.g., the slow map tasks all had the same Input Split).           
         The situation came to a head during a distributed copy, when the copy was severely prolonged by the lagging node.
		</p></div><div class="section" title="1.3.1.2.&nbsp;Hardware"><div class="titlepage"><div><div><h4 class="title"><a name="d816e87"></a>1.3.1.2.&nbsp;Hardware</h4></div></div></div><p>Datanodes:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Two 12-core processors</li><li class="listitem">Six Enerprise SATA disks</li><li class="listitem">24GB of RAM</li><li class="listitem">Two bonded gigabit NICs</li></ul></div><p>
        </p><p>Network:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">10 Gigabit top-of-rack switches</li><li class="listitem">20 Gigabit bonded interconnects between racks.</li></ul></div><p>
        </p></div><div class="section" title="1.3.1.3.&nbsp;Hypotheses"><div class="titlepage"><div><div><h4 class="title"><a name="d816e110"></a>1.3.1.3.&nbsp;Hypotheses</h4></div></div></div><div class="section" title="1.3.1.3.1.&nbsp;HBase &#34;Hot Spot&#34; Region"><div class="titlepage"><div><div><h5 class="title"><a name="d816e113"></a>1.3.1.3.1.&nbsp;HBase "Hot Spot" Region</h5></div></div></div><p>We hypothesized that we were experiencing a familiar point of pain: a "hot spot" region in an HBase table, 
		  where uneven key-space distribution can funnel a huge number of requests to a single HBase region, bombarding the RegionServer 
		  process and cause slow response time. Examination of the HBase Master status page showed that the number of HBase requests to the 
		  troubled node was almost zero.  Further, examination of the HBase logs showed that there were no region splits, compactions, or other region transitions 
		  in progress.  This effectively ruled out a "hot spot" as the root cause of the observed slowness.
          </p></div><div class="section" title="1.3.1.3.2.&nbsp;HBase Region With Non-Local Data"><div class="titlepage"><div><div><h5 class="title"><a name="d816e118"></a>1.3.1.3.2.&nbsp;HBase Region With Non-Local Data</h5></div></div></div><p>Our next hypothesis was that one of the MapReduce tasks was requesting data from HBase that was not local to the datanode, thus 
		  forcing HDFS to request data blocks from other servers over the network.  Examination of the datanode logs showed that there were very 
		  few blocks being requested over the network, indicating that the HBase region was correctly assigned, and that the majority of the necessary 
		  data was located on the node. This ruled out the possibility of non-local data causing a slowdown.
          </p></div><div class="section" title="1.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk"><div class="titlepage"><div><div><h5 class="title"><a name="d816e123"></a>1.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk</h5></div></div></div><p>After concluding that the Hadoop and HBase were not likely to be the culprits, we moved on to troubleshooting the datanode's hardware. 
          Java, by design, will periodically scan its entire memory space to do garbage collection.  If system memory is heavily overcommitted, the Linux 
          kernel may enter a vicious cycle, using up all of its resources swapping Java heap back and forth from disk to RAM as Java tries to run garbage 
          collection.  Further, a failing hard disk will often retry reads and/or writes many times before giving up and returning an error. This can manifest 
          as high iowait, as running processes wait for reads and writes to complete.  Finally, a disk nearing the upper edge of its performance envelope will 
          begin to cause iowait as it informs the kernel that it cannot accept any more data, and the kernel queues incoming data into the dirty write pool in memory.  
          However, using <code class="code">vmstat(1)</code> and <code class="code">free(1)</code>, we could see that no swap was being used, and the amount of disk IO was only a few kilobytes per second.
          </p></div><div class="section" title="1.3.1.3.4.&nbsp;Slowness Due To High Processor Usage"><div class="titlepage"><div><div><h5 class="title"><a name="d816e134"></a>1.3.1.3.4.&nbsp;Slowness Due To High Processor Usage</h5></div></div></div><p>Next, we checked to see whether the system was performing slowly simply due to very high computational load.  <code class="code">top(1)</code> showed that the system load 
          was higher than normal, but <code class="code">vmstat(1)</code> and <code class="code">mpstat(1)</code> showed that the amount of processor being used for actual computation was low.
          </p></div><div class="section" title="1.3.1.3.5.&nbsp;Network Saturation (The Winner)"><div class="titlepage"><div><div><h5 class="title"><a name="d816e148"></a>1.3.1.3.5.&nbsp;Network Saturation (The Winner)</h5></div></div></div><p>Since neither the disks nor the processors were being utilized heavily, we moved on to the performance of the network interfaces.  The datanode had two 
          gigabit ethernet adapters, bonded to form an active-standby interface.  <code class="code">ifconfig(8)</code> showed some unusual anomalies, namely interface errors, overruns, framing errors. 
          While not unheard of, these kinds of errors are exceedingly rare on modern hardware which is operating as it should:
</p><pre class="programlisting">		
$ /sbin/ifconfig bond0
bond0  Link encap:Ethernet  HWaddr 00:00:00:00:00:00  
inet addr:10.x.x.x  Bcast:10.x.x.255  Mask:255.255.255.0
UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
RX packets:2990700159 errors:12 dropped:0 overruns:1 frame:6          &lt;--- Look Here! Errors!
TX packets:3443518196 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0 
RX bytes:2416328868676 (2.4 TB)  TX bytes:3464991094001 (3.4 TB)
</pre><p>
          </p><p>These errors immediately lead us to suspect that one or more of the ethernet interfaces might have negotiated the wrong line speed.  This was confirmed both by running an ICMP ping 
          from an external host and observing round-trip-time in excess of 700ms, and by running <code class="code">ethtool(8)</code> on the members of the bond interface and discovering that the active interface 
          was operating at 100Mbs/, full duplex.
</p><pre class="programlisting">		
$ sudo ethtool eth0
Settings for eth0:
Supported ports: [ TP ]
Supported link modes:   10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Supports auto-negotiation: Yes
Advertised link modes:  10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Advertised pause frame use: No
Advertised auto-negotiation: Yes
Link partner advertised link modes:  Not reported
Link partner advertised pause frame use: No
Link partner advertised auto-negotiation: No
Speed: 100Mb/s                                     &lt;--- Look Here!  Should say 1000Mb/s!
Duplex: Full
Port: Twisted Pair
PHYAD: 1
Transceiver: internal
Auto-negotiation: on
MDI-X: Unknown
Supports Wake-on: umbg
Wake-on: g
Current message level: 0x00000003 (3)
Link detected: yes
</pre><p>		
		  </p><p>In normal operation, the ICMP ping round trip time should be around 20ms, and the interface speed and duplex should read, "1000MB/s", and, "Full", respectively.  
		  </p></div></div><div class="section" title="1.3.1.4.&nbsp;Resolution"><div class="titlepage"><div><div><h4 class="title"><a name="d816e169"></a>1.3.1.4.&nbsp;Resolution</h4></div></div></div><p>After determining that the active ethernet adapter was at the incorrect speed, we used the <code class="code">ifenslave(8)</code> command to make the standby interface 
   	  the active interface, which yielded an immediate improvement in MapReduce performance, and a 10 times improvement in network throughput:
	  </p><p>On the next trip to the datacenter, we determined that the line speed issue was ultimately caused by a bad network cable, which was replaced.
	  </p></div></div><div class="section" title="1.3.2.&nbsp;Case Study #2 (Performance Research 2012)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.1"></a>1.3.2.&nbsp;Case Study #2 (Performance Research 2012)</h3></div></div></div><p>Investigation results of a self-described "we're not sure what's wrong, but it seems slow" problem. 
      <a class="link" href="http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html" target="_top">http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html</a>
      </p></div><div class="section" title="1.3.3.&nbsp;Case Study #3 (Performance Research 2010))"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.2"></a>1.3.3.&nbsp;Case Study #3 (Performance Research 2010))</h3></div></div></div><p>
      Investigation results of general cluster performance from 2010.  Although this research is on an older version of the codebase, this writeup
      is still very useful in terms of approach.
      <a class="link" href="http://hstack.org/hbase-performance-testing/" target="_top">http://hstack.org/hbase-performance-testing/</a>
      </p></div><div class="section" title="1.3.4.&nbsp;Case Study #4 (xcievers Config)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.xceivers"></a>1.3.4.&nbsp;Case Study #4 (xcievers Config)</h3></div></div></div><p>Case study of configuring <code class="code">xceivers</code>, and diagnosing errors from mis-configurations.
      <a class="link" href="http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html" target="_top">http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html</a>
      </p><p>See also <a class="xref" href="#">???</a>.
      </p></div></div></div></body></html>