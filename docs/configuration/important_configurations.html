<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>1.5.&nbsp;The Important Configurations</title><link rel="stylesheet" type="text/css" href="../css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"><link rel="home" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Apache HBase (TM) Configuration"><link rel="up" href="configuration.html" title="Chapter&nbsp;1.&nbsp;Apache HBase (TM) Configuration"><link rel="prev" href="example_config.html" title="1.4.&nbsp;Example Configurations"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">1.5.&nbsp;The Important Configurations</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="example_config.html">Prev</a>&nbsp;</td><th width="60%" align="center">&nbsp;</th><td width="20%" align="right">&nbsp;</td></tr></table><hr></div><div class="section" title="1.5.&nbsp;The Important Configurations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="important_configurations"></a>1.5.&nbsp;The Important Configurations</h2></div></div></div><p>Below we list what the <span class="emphasis"><em>important</em></span>
      Configurations.  We've divided this section into
      required configuration and worth-a-look recommended configs.
      </p><div class="section" title="1.5.1.&nbsp;Required Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="required_configuration"></a>1.5.1.&nbsp;Required Configurations</h3></div></div></div><p>Review the <a class="xref" href="configuration.html#os" title="1.1.2.&nbsp;Operating System">Section&nbsp;1.1.2, &#8220;Operating System&#8221;</a> and <a class="xref" href="configuration.html#hadoop" title="1.1.3.&nbsp;Hadoop">Section&nbsp;1.1.3, &#8220;Hadoop&#8221;</a> sections.
      </p><div class="section" title="1.5.1.1.&nbsp;Big Cluster Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="big.cluster.config"></a>1.5.1.1.&nbsp;Big Cluster Configurations</h4></div></div></div><p>If a cluster with a lot of regions, it is possible if an eager beaver
            regionserver checks in soon after master start while all the rest in the
            cluster are laggardly, this first server to checkin will be assigned all
            regions.  If lots of regions, this first server could buckle under the
            load.  To prevent the above scenario happening up the
            <code class="varname">hbase.master.wait.on.regionservers.mintostart</code> from its
            default value of 1.  See
            <a class="link" href="https://issues.apache.org/jira/browse/HBASE-6389" target="_top">HBASE-6389 Modify the conditions to ensure that Master waits for sufficient number of Region Servers before starting region assignments</a>
            for more detail.
        </p></div></div><div class="section" title="1.5.2.&nbsp;Recommended Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="recommended_configurations"></a>1.5.2.&nbsp;Recommended Configurations</h3></div></div></div><div class="section" title="1.5.2.1.&nbsp;ZooKeeper Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="recommended_configurations.zk"></a>1.5.2.1.&nbsp;ZooKeeper Configuration</h4></div></div></div><div class="section" title="1.5.2.1.1.&nbsp;zookeeper.session.timeout"><div class="titlepage"><div><div><h5 class="title"><a name="zookeeper.session.timeout"></a>1.5.2.1.1.&nbsp;<code class="varname">zookeeper.session.timeout</code></h5></div></div></div><p>The default timeout is three minutes (specified in milliseconds). This means
              that if a server crashes, it will be three minutes before the Master notices
              the crash and starts recovery. You might like to tune the timeout down to
              a minute or even less so the Master notices failures the sooner.
              Before changing this value, be sure you have your JVM garbage collection
              configuration under control otherwise, a long garbage collection that lasts
              beyond the ZooKeeper session timeout will take out
              your RegionServer (You might be fine with this -- you probably want recovery to start
          on the server if a RegionServer has been in GC for a long period of time).</p><p>To change this configuration, edit <code class="filename">hbase-site.xml</code>,
          copy the changed file around the cluster and restart.</p><p>We set this value high to save our having to field noob questions up on the mailing lists asking
              why a RegionServer went down during a massive import.  The usual cause is that their JVM is untuned and
              they are running into long GC pauses.  Our thinking is that
              while users are  getting familiar with HBase, we'd save them having to know all of its
              intricacies.  Later when they've built some confidence, then they can play
              with configuration such as this.
          </p></div><div class="section" title="1.5.2.1.2.&nbsp;Number of ZooKeeper Instances"><div class="titlepage"><div><div><h5 class="title"><a name="zookeeper.instances"></a>1.5.2.1.2.&nbsp;Number of ZooKeeper Instances</h5></div></div></div><p>See <a class="xref" href="">???</a>.
          </p></div></div><div class="section" title="1.5.2.2.&nbsp;HDFS Configurations"><div class="titlepage"><div><div><h4 class="title"><a name="recommended.configurations.hdfs"></a>1.5.2.2.&nbsp;HDFS Configurations</h4></div></div></div><div class="section" title="1.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated"><div class="titlepage"><div><div><h5 class="title"><a name="dfs.datanode.failed.volumes.tolerated"></a>1.5.2.2.1.&nbsp;dfs.datanode.failed.volumes.tolerated</h5></div></div></div><p>This is the "...number of volumes that are allowed to fail before a datanode stops offering service. By default
                  any volume failure will cause a datanode to shutdown" from the <code class="filename">hdfs-default.xml</code>
                  description.  If you have &gt; three or four disks, you might want to set this to 1 or if you have many disks,
                  two or more.
              </p></div></div><div class="section" title="1.5.2.3.&nbsp;hbase.regionserver.handler.count"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.handler.count"></a>1.5.2.3.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h4></div></div></div><p>
          This setting defines the number of threads that are kept open to answer
          incoming requests to user tables. The default of 10 is rather low in order to
          prevent users from killing their region servers when using large write buffers
          with a high number of concurrent clients. The rule of thumb is to keep this
          number low when the payload per request approaches the MB (big puts, scans using
          a large cache) and high when the payload is small (gets, small puts, ICVs, deletes).
          </p><p>
          It is safe to set that number to the
          maximum number of incoming clients if their payload is small, the typical example
          being a cluster that serves a website since puts aren't typically buffered
          and most of the operations are gets.
          </p><p>
          The reason why it is dangerous to keep this setting high is that the aggregate
          size of all the puts that are currently happening in a region server may impose
          too much pressure on its memory, or even trigger an OutOfMemoryError. A region server
          running on low memory will trigger its JVM's garbage collector to run more frequently
          up to a point where GC pauses become noticeable (the reason being that all the memory
          used to keep all the requests' payloads cannot be trashed, no matter how hard the
          garbage collector tries). After some time, the overall cluster
          throughput is affected since every request that hits that region server will take longer,
          which exacerbates the problem even more.
          </p><p>You can get a sense of whether you have too little or too many handlers by
            <a class="xref" href="">???</a>
            on an individual RegionServer then tailing its logs (Queued requests
            consume memory).
            </p></div><div class="section" title="1.5.2.4.&nbsp;Configuration for large memory machines"><div class="titlepage"><div><div><h4 class="title"><a name="big_memory"></a>1.5.2.4.&nbsp;Configuration for large memory machines</h4></div></div></div><p>
          HBase ships with a reasonable, conservative configuration that will
          work on nearly all
          machine types that people might want to test with. If you have larger
          machines -- HBase has 8G and larger heap -- you might the following configuration options helpful.
          TODO.
        </p></div><div class="section" title="1.5.2.5.&nbsp;Compression"><div class="titlepage"><div><div><h4 class="title"><a name="config.compression"></a>1.5.2.5.&nbsp;Compression</h4></div></div></div><p>You should consider enabling ColumnFamily compression.  There are several options that are near-frictionless and in most all cases boost
      performance by reducing the size of StoreFiles and thus reducing I/O.
      </p><p>See <a class="xref" href="">???</a> for more information.</p></div><div class="section" title="1.5.2.6.&nbsp;Bigger Regions"><div class="titlepage"><div><div><h4 class="title"><a name="bigger.regions"></a>1.5.2.6.&nbsp;Bigger Regions</h4></div></div></div><p>
      Consider going to larger regions to cut down on the total number of regions
      on your cluster. Generally less Regions to manage makes for a smoother running
      cluster (You can always later manually split the big Regions should one prove
      hot and you want to spread the request load over the cluster).  A lower number of regions is
       preferred, generally in the range of 20 to low-hundreds
       per RegionServer.  Adjust the regionsize as appropriate to achieve this number.
       </p><p>For the 0.90.x codebase, the upper-bound of regionsize is about 4Gb, with a default of 256Mb.
       For 0.92.x codebase, due to the HFile v2 change much larger regionsizes can be supported (e.g., 20Gb).
       </p><p>You may need to experiment with this setting based on your hardware configuration and application needs.
       </p><p>Adjust <code class="code">hbase.hregion.max.filesize</code> in your <code class="filename">hbase-site.xml</code>.
       RegionSize can also be set on a per-table basis via
       <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.
      </p><div class="section" title="1.5.2.6.1.&nbsp;How many regions per RegionServer?"><div class="titlepage"><div><div><h5 class="title"><a name="too_many_regions"></a>1.5.2.6.1.&nbsp;How many regions per RegionServer?</h5></div></div></div><p>
              Typically you want to keep your region count low on HBase for numerous reasons.
              Usually right around 100 regions per RegionServer has yielded the best results.
              Here are some of the reasons below for keeping region count low:
              <span style="color: red">&lt;unorderedlist&gt;
                  <span style="color: red">&lt;listitem&gt;<p>
                          MSLAB requires 2mb per memstore (that's 2mb per family per region).
                          1000 regions that have 2 families each is 3.9GB of heap used, and it's not even storing data yet. NB: the 2MB value is configurable.
                  </p>&lt;/listitem&gt;</span>
                  <span style="color: red">&lt;listitem&gt;<p>If you fill all the regions at somewhat the same rate, the global memory usage makes it that it forces tiny
                          flushes when you have too many regions which in turn generates compactions.
                          Rewriting the same data tens of times is the last thing you want.
                          An example is filling 1000 regions (with one family) equally and let's consider a lower bound for global memstore
                          usage of 5GB (the region server would have a big heap).
                          Once it reaches 5GB it will force flush the biggest region,
                          at that point they should almost all have about 5MB of data so
                          it would flush that amount. 5MB inserted later, it would flush another
                          region that will now have a bit over 5MB of data, and so on.
                          A basic formula for the amount of regions to have per region server would
                          look like this:
                          Heap * upper global memstore limit = amount of heap devoted to memstore
                          then the amount of heap devoted to memstore / (Number of regions per RS * CFs).
                          This will give you the rough memstore size if everything is being written to.
                          A more accurate formula is
                          Heap * upper global memstore limit = amount of heap devoted to memstore then the
                          amount of heap devoted to memstore / (Number of actively written regions per RS * CFs).
                          This can allot you a higher region count from the write perspective if you know how many
                          regions you will be writing to at one time.
                  </p>&lt;/listitem&gt;</span>
                  <span style="color: red">&lt;listitem&gt;<p>The master as is is allergic to tons of regions, and will
                          take a lot of time assigning them and moving them around in batches.
                          The reason is that it's heavy on ZK usage, and it's not very async
                          at the moment (could really be improved -- and has been imporoved a bunch
                          in 0.96 hbase).
                  </p>&lt;/listitem&gt;</span>
                  <span style="color: red">&lt;listitem&gt;<p>
                          In older versions of HBase (pre-v2 hfile, 0.90 and previous), tons of regions
                          on a few RS can cause the store file index to rise raising heap usage and can
                          create memory pressure or OOME on the RSs
                  </p>&lt;/listitem&gt;</span>
          &lt;/unorderedlist&gt;</span>
      </p><p>Another issue is the effect of the number of regions on mapreduce jobs.
          Keeping 5 regions per RS would be too low for a job, whereas 1000 will generate too many maps.
      </p></div></div><div class="section" title="1.5.2.7.&nbsp;Managed Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="disable.splitting"></a>1.5.2.7.&nbsp;Managed Splitting</h4></div></div></div><p>
      Rather than let HBase auto-split your Regions, manage the splitting manually
      <sup>[<a name="d73e2450" href="#ftn.d73e2450" class="footnote">11</a>]</sup>.
 With growing amounts of data, splits will continually be needed. Since
 you always know exactly what regions you have, long-term debugging and
 profiling is much easier with manual splits. It is hard to trace the logs to
 understand region level problems if it keeps splitting and getting renamed.
 Data offlining bugs + unknown number of split regions == oh crap! If an
 <code class="classname">HLog</code> or <code class="classname">StoreFile</code>
 was mistakenly unprocessed by HBase due to a weird bug and
 you notice it a day or so later, you can be assured that the regions
 specified in these files are the same as the current regions and you have
 less headaches trying to restore/replay your data.
 You can finely tune your compaction algorithm. With roughly uniform data
 growth, it's easy to cause split / compaction storms as the regions all
 roughly hit the same data size at the same time. With manual splits, you can
 let staggered, time-based major compactions spread out your network IO load.
      </p><p>
 How do I turn off automatic splitting? Automatic splitting is determined by the configuration value
 <code class="code">hbase.hregion.max.filesize</code>. It is not recommended that you set this
 to <code class="varname">Long.MAX_VALUE</code> in case you forget about manual splits. A suggested setting
 is 100GB, which would result in &gt; 1hr major compactions if reached.
 </p><p>What's the optimal number of pre-split regions to create?
 Mileage will vary depending upon your application.
 You could start low with 10 pre-split regions / server and watch as data grows
 over time. It's better to err on the side of too little regions and rolling split later.
 A more complicated answer is that this depends upon the largest storefile
 in your region. With a growing data size, this will get larger over time. You
 want the largest region to be just big enough that the <code class="classname">Store</code> compact
 selection algorithm only compacts it due to a timed major. If you don't, your
 cluster can be prone to compaction storms as the algorithm decides to run
 major compactions on a large series of regions all at once. Note that
 compaction storms are due to the uniform data growth, not the manual split
 decision.
 </p><p> If you pre-split your regions too thin, you can increase the major compaction
interval by configuring <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code>. If your data size
grows too large, use the (post-0.90.0 HBase) <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code>
script to perform a network IO safe rolling split
of all regions.
</p></div><div class="section" title="1.5.2.8.&nbsp;Managed Compactions"><div class="titlepage"><div><div><h4 class="title"><a name="managed.compactions"></a>1.5.2.8.&nbsp;Managed Compactions</h4></div></div></div><p>A common administrative technique is to manage major compactions manually, rather than letting
      HBase do it.  By default, <code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code> is one day and major compactions
      may kick in when you least desire it - especially on a busy system.  To turn off automatic major compactions set
      the value to <code class="varname">0</code>.
      </p><p>It is important to stress that major compactions are absolutely necessary for StoreFile cleanup, the only variant is when
      they occur.  They can be administered through the HBase shell, or via
      <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin</a>.
      </p><p>For more information about compactions and the compaction file selection process, see <a class="xref" href="">???</a></p></div><div class="section" title="1.5.2.9.&nbsp;Speculative Execution"><div class="titlepage"><div><div><h4 class="title"><a name="spec.ex"></a>1.5.2.9.&nbsp;Speculative Execution</h4></div></div></div><p>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it is generally advised to turn off
        Speculative Execution at a system-level unless you need it for a specific case, where it can be configured per-job.
        Set the properties <code class="varname">mapred.map.tasks.speculative.execution</code> and
        <code class="varname">mapred.reduce.tasks.speculative.execution</code> to false.
        </p></div></div><div class="section" title="1.5.3.&nbsp;Other Configurations"><div class="titlepage"><div><div><h3 class="title"><a name="other_configuration"></a>1.5.3.&nbsp;Other Configurations</h3></div></div></div><div class="section" title="1.5.3.1.&nbsp;Balancer"><div class="titlepage"><div><div><h4 class="title"><a name="balancer_config"></a>1.5.3.1.&nbsp;Balancer</h4></div></div></div><p>The balancer is a periodic operation which is run on the master to redistribute regions on the cluster.  It is configured via
           <code class="varname">hbase.balancer.period</code> and defaults to 300000 (5 minutes). </p><p>See <a class="xref" href="">???</a> for more information on the LoadBalancer.
           </p></div><div class="section" title="1.5.3.2.&nbsp;Disabling Blockcache"><div class="titlepage"><div><div><h4 class="title"><a name="disabling.blockcache"></a>1.5.3.2.&nbsp;Disabling Blockcache</h4></div></div></div><p>Do not turn off block cache (You'd do it by setting <code class="varname">hbase.block.cache.size</code> to zero).
           Currently we do not do well if you do this because the regionserver will spend all its time loading hfile
           indices over and over again.  If your working set it such that block cache does you no good, at least
           size the block cache such that hfile indices will stay up in the cache (you can get a rough idea
           on the size you need by surveying regionserver UIs; you'll see index block size accounted near the
           top of the webpage).</p></div><div class="section" title="1.5.3.3.&nbsp;Nagle's or the small package problem"><div class="titlepage"><div><div><h4 class="title"><a name="nagles"></a>1.5.3.3.&nbsp;<a class="link" href="http://en.wikipedia.org/wiki/Nagle's_algorithm" target="_top">Nagle's</a> or the small package problem</h4></div></div></div><p>If a big 40ms or so occasional delay is seen in operations against HBase,
      try the Nagles' setting.  For example, see the user mailing list thread,
      <a class="link" href="http://search-hadoop.com/m/pduLg2fydtE/Inconsistent+scan+performance+with+caching+set+&amp;subj=Re+Inconsistent+scan+performance+with+caching+set+to+1" target="_top">Inconsistent scan performance with caching set to 1</a>
      and the issue cited therein where setting notcpdelay improved scan speeds.  You might also
      see the graphs on the tail of <a class="link" href="https://issues.apache.org/jira/browse/HBASE-7008" target="_top">HBASE-7008 Set scanner caching to a better default</a>
      where our Lars Hofhansl tries various data sizes w/ Nagle's on and off measuring the effect.</p></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d73e2450" href="#d73e2450" class="para">11</a>] </sup>What follows is taken from the javadoc at the head of
      the <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code> tool
      added to HBase post-0.90.0 release.
      </p></div></div></div><div id="disqus_thread"></div><script type="text/javascript">
    var disqus_shortname = 'hbase'; // required: replace example with your forum shortname
    var disqus_url = 'http://hbase.apache.org/book';
    var disqus_identifier = 'important_configurations';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a><div class="navfooter"><hr><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="example_config.html">Prev</a>&nbsp;</td><td width="20%" align="center">&nbsp;</td><td width="40%" align="right">&nbsp;</td></tr><tr><td width="40%" align="left" valign="top">1.4.&nbsp;Example Configurations&nbsp;</td><td width="20%" align="center"><a accesskey="h" href="configuration.html">Home</a></td><td width="40%" align="right" valign="top">&nbsp;</td></tr></table></div></body></html>